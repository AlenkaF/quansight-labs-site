<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Quansight Labs (Posts about github-actions)</title><link>https://labs.quansight.org/</link><description></description><atom:link href="https://labs.quansight.org/categories/github-actions.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2021 &lt;a href="mailto:info@quansight.com"&gt;Quansight Labs Team&lt;/a&gt; </copyright><lastBuildDate>Sun, 29 Aug 2021 10:45:57 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Is GitHub Actions suitable for running benchmarks?</title><link>https://labs.quansight.org/blog/2021/08/github-actions-benchmarks/</link><dc:creator>Jaime Rodríguez-Guerra</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;img alt="Reliability of benchmarks in GitHub Actions. This 2D plot shows a 16-day timeseries in the X axis.
  Each data point in the X axis corresponds to a cloud of 75 measurements (one per benchmark test).
  The y-axis spread of each cloud corresponds to the performance ratio. Ideal measurements would have
  a performance ratio of 1.0, since both runs returned the exact same performance. In practice this
  does not happen and we can observe ratios between 0.6 and 1.5. This plot shows that while there
  is an observable y-spread, it is small enough to be considered sensitive to performance
  regressions of more than 50%." src="https://labs.quansight.org/images/2021/08/github-actions-benchmark.png"&gt;&lt;/p&gt;
&lt;p&gt;Benchmarking software is a tricky business. For robust results, you need dedicated
hardware that only runs the benchmarking suite under controlled conditions. No other
processes! No OS updates! Nothing else! Even then, you might find out that CPU throttling,
thermal regulation and other issues can introduce noise in your measurements.&lt;/p&gt;
&lt;p&gt;So, how are we even trying to do it on a CI provider like GitHub Actions?
Every job runs in a separate VM instance with frequent updates and shared resources. It
looks like it would just be a very expensive random number generator.&lt;/p&gt;
&lt;p&gt;Well, it turns out that there &lt;em&gt;is&lt;/em&gt; a sensible way to do it: &lt;strong&gt;relative benchmarking&lt;/strong&gt;.
And we know it works because we have been collecting stability data points for several
weeks.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2021/08/github-actions-benchmarks/"&gt;Read more…&lt;/a&gt; (13 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>continuous-integration</category><category>github-actions</category><category>Open-Source</category><category>performance</category><guid>https://labs.quansight.org/blog/2021/08/github-actions-benchmarks/</guid><pubDate>Wed, 18 Aug 2021 00:01:00 GMT</pubDate></item></channel></rss>